{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66d9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup and splinter\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from splinter import Browser\n",
    "\n",
    "# Import other dependencies\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Import geoapify api key\n",
    "from config import geoapify_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accadb89",
   "metadata": {},
   "source": [
    "### Scraping Top 25 (page each for 4 common page layout categories  - Hotels, Beaches, Restaurants & Things to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bfeaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoapify url endpoint\n",
    "base_url = \"https://api.geoapify.com/v1/geocode/search\"\n",
    "\n",
    "# List of urls and category used for scraping\n",
    "url_list = []\n",
    "category_list = []\n",
    "\n",
    "url_list.append('https://www.tripadvisor.com/TravelersChoice-Beaches')\n",
    "category_list.append('beaches')\n",
    "\n",
    "url_list.append(\"https://www.tripadvisor.com/TravelersChoice-ThingsToDo\")\n",
    "category_list.append(\"things\")\n",
    "\n",
    "url_list.append(\"https://www.tripadvisor.com/TravelersChoice-Restaurants\")\n",
    "category_list.append(\"restaurants\")\n",
    "\n",
    "url_list.append(\"https://www.tripadvisor.com/TravelersChoice-Hotels\")\n",
    "category_list.append(\"hotels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c41c9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File static\\data\\beaches.csv already extracted. Skipping scraping\n",
      "File static\\data\\things.csv already extracted. Skipping scraping\n",
      "File static\\data\\restaurants.csv already extracted. Skipping scraping\n"
     ]
    }
   ],
   "source": [
    "# loop thru url list that we need to scrape and get data (done only if csv of data is not present)\n",
    "\n",
    "for index, item in enumerate(url_list):\n",
    "    \n",
    "    search_file = \"static/data/\" + category_list[index] + \".csv\"\n",
    "    csv_file = Path(search_file)\n",
    "\n",
    "    if csv_file.exists():\n",
    "        print(f\"File {csv_file} already extracted. Skipping scraping\")\n",
    "    else:\n",
    "                \n",
    "        # Set up Splinter\n",
    "        browser = Browser('chrome')\n",
    "\n",
    "        # Visit the website - sleep provided to avoid continuous calls   \n",
    "        browser.visit(item)\n",
    "        time.sleep(30)       \n",
    "        # Optional delay for loading the page\n",
    "        browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "        # Scrape the website html\n",
    "        html = browser.html\n",
    "\n",
    "        # Create a BeautifulSoup object from the scraped HTML\n",
    "        data = soup(html, 'html.parser')\n",
    "\n",
    "        # Get Rank and name\n",
    "        names = data.find_all(class_=\"mainName extra\")\n",
    "        namelist = []\n",
    "        ranklist = []\n",
    "        parts = []\n",
    "        for i in names:\n",
    "            href=i.find(\"a\")    \n",
    "            text_data=href.text   \n",
    "            parts = text_data.split('.')    \n",
    "            ranklist.append(parts[0])\n",
    "            namelist.append(parts[1].strip())\n",
    "    \n",
    "        # Get City and Country (single string)\n",
    "        cities = data.find_all(class_=\"smaller\")\n",
    "        citylist = []\n",
    "        lat = []\n",
    "        lon = []\n",
    "        for i in cities:\n",
    "            href=i.find(\"a\")\n",
    "            city_loc = href.text    \n",
    "            citylist.append(city_loc)\n",
    "            \n",
    "            # call geoapify to get lat/lon for location\n",
    "            params = {\n",
    "                \"text\": city_loc,\n",
    "                \"apiKey\": geoapify_key }\n",
    "            # Run request\n",
    "            response = requests.get(base_url, params=params).json()\n",
    "            \n",
    "            # Extract lat/lon\n",
    "            latitude = response[\"features\"][0][\"properties\"][\"lat\"]\n",
    "            longitude = response[\"features\"][0][\"properties\"][\"lon\"]\n",
    "\n",
    "            # append to list\n",
    "            lat.append(latitude)\n",
    "            lon.append(longitude)\n",
    "    \n",
    "\n",
    "        # Get image urls\n",
    "        images = data.find_all(class_=\"sizedThumb_container\")\n",
    "        iurls = []\n",
    "        for i in images:\n",
    "            href=i.find(\"img\")\n",
    "            iurls.append(href[\"src\"])\n",
    "\n",
    "        # Get description of restaurant/a customer review \n",
    "        desclist = []\n",
    "        quot_tags = data.find_all(class_=\"quot\")\n",
    "        # removing classes with \"quot quot2\" used for second quote as we need just 1 quote per target\n",
    "        quot_tags = [tag for tag in quot_tags if 'quot2' not in ''.join(tag['class'])]\n",
    "        for texts in quot_tags:\n",
    "            x = texts.find(\"i\").next_sibling.strip()        \n",
    "            desclist.append(x)\n",
    "\n",
    "        # Get url to go retrieve rating and reviews - this will be used later to get details\n",
    "        url_ary = []\n",
    "        for lnk in data.find_all(class_=\"firstone\"):\n",
    "            href=lnk.find(\"a\")    \n",
    "            url_ary.append(href['href'])\n",
    "\n",
    "        # Close browser\n",
    "        browser.quit() \n",
    "\n",
    "        # Create array with category and count of rank array\n",
    "        cat_list = np.repeat([category_list[index]], len(ranklist))\n",
    "\n",
    "        # Load arrays as columns of dataframe\n",
    "        df = pd.DataFrame({\"category\": cat_list, \"rank\": ranklist, \"name\": namelist, \"location\": citylist, \n",
    "                       \"imageurl\": iurls, \"description\": desclist, \"latitude\": lat, \"longitude\": lon, \"ratingurl\": url_ary})\n",
    "        \n",
    "        # save dataframe as csv \n",
    "        filename = \"static/data/\" + category_list[index] + \".csv\"\n",
    "        df.to_csv(filename, encoding=\"utf-8\", index=False, header=True)\n",
    "        \n",
    "\n",
    "    # sleep for a minute before calling next url (providing break to avoid continuous pings to website)\n",
    "    time.sleep(10)\n",
    "\n",
    "# Final checks\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600a27d",
   "metadata": {},
   "source": [
    "### Scrape Destinations - missing some of the required elements so handled separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3e7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dest = \"https://www.tripadvisor.com/TravelersChoice-Destinations\"\n",
    "cat_dest = \"destinations\"\n",
    "category_list.append(\"destinations\")\n",
    "    \n",
    "search_file = \"static/data/\" + cat_dest + \".csv\"\n",
    "csv_file = Path(search_file)\n",
    "\n",
    "if csv_file.exists():\n",
    "    print(f\"File {csv_file} already extracted. Skipping scraping\")\n",
    "else:\n",
    "            \n",
    "    # Set up Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url_dest)\n",
    "      \n",
    "    # Optional delay for loading the page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    data = soup(html, 'html.parser')\n",
    "    \n",
    "    # Close browser\n",
    "    browser.quit() \n",
    "\n",
    "    # Get Rank and name  (City same as name)\n",
    "    names = data.find_all(class_=\"mainName\")\n",
    "    citylist = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    namelist = []\n",
    "    ranklist = []\n",
    "    parts = []\n",
    "    for i in names:\n",
    "        href=i.find(\"a\")    \n",
    "        text_data=href.text   \n",
    "        parts = text_data.split('.')    \n",
    "        ranklist.append(parts[0])\n",
    "        namelist.append(parts[1].strip())\n",
    "        \n",
    "        city_loc = (parts[1].strip())\n",
    "        citylist.append(city_loc)    \n",
    "        # call geoapify to get lat/lon for location\n",
    "        params = {\n",
    "            \"text\": city_loc,\n",
    "            \"apiKey\": geoapify_key }\n",
    "        # Run request\n",
    "        response = requests.get(base_url, params=params).json()\n",
    "            \n",
    "        # Extract lat/lon\n",
    "        latitude = response[\"features\"][0][\"properties\"][\"lat\"]\n",
    "        longitude = response[\"features\"][0][\"properties\"][\"lon\"]\n",
    "\n",
    "        # append to list\n",
    "        lat.append(latitude)\n",
    "        lon.append(longitude)    \n",
    "\n",
    "    # Get image urls\n",
    "    images = data.find_all(class_=\"sizedThumb_container\")\n",
    "    iurls = []\n",
    "    for i in images:\n",
    "        href=i.find(\"img\")\n",
    "        iurls.append(href[\"src\"])    \n",
    "    \n",
    "    # Get url to go retrieve rating and reviews - destinations is new so capture for df to csv load (data not used)\n",
    "    url_ary = []\n",
    "    desclist = []\n",
    "    for lnk in data.find_all(class_=\"firstone\"):\n",
    "        href=lnk.find(\"a\")    \n",
    "        url_ary.append(href['href'])\n",
    "\n",
    "        # Use rating url to get to next page and get description - rating not present due to this being a relatively new category\n",
    "        desc_url = \"https://www.tripadvisor.com\" + href['href']\n",
    "        # setup Splinter\n",
    "        browser = Browser('chrome')\n",
    "\n",
    "        # Visit the website - sleep provided to avoid continuous calls   \n",
    "        browser.visit(desc_url)\n",
    "        # Scrape the website html\n",
    "        html = browser.html\n",
    "\n",
    "        # Create a BeautifulSoup object from the scraped HTML\n",
    "        raters = soup(html, 'html.parser')\n",
    "        browser.quit()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Get desciption from here\n",
    "        desc_text = raters.find(class_=\"GYFPJ wESPJ _J B- G- Wh _S\")\n",
    "        desclist.append(desc_text.text)\n",
    "\n",
    "    \n",
    "    # Create array with category and count of rank array\n",
    "    cat_list = np.repeat([cat_dest], len(ranklist))\n",
    "\n",
    "    # Load arrays as columns of dataframe\n",
    "    df = pd.DataFrame({\"category\": cat_list, \"rank\": ranklist, \"name\": namelist, \"location\": citylist, \n",
    "                  \"imageurl\": iurls, \"description\": desclist, \"latitude\": lat, \"longitude\": lon, \"ratingurl\": url_ary})\n",
    "        \n",
    "    # save dataframe as csv \n",
    "    filename = \"static/data/\" + cat_dest + \".csv\"\n",
    "    df.to_csv(filename, encoding=\"utf-8\", index=False, header=True)  \n",
    "\n",
    "# Final checks\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45544023",
   "metadata": {},
   "source": [
    "### Scraping Reviews for Beaches Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "rvcount = []\n",
    "exclnt = []\n",
    "vgood = []\n",
    "averg = []\n",
    "poor = []\n",
    "trrbl = []\n",
    "rank_key = []\n",
    "\n",
    "# Get rating and misc info for all beaches\n",
    "prefix = \"https://www.tripadvisor.com\"\n",
    "\n",
    "beaches_df = pd.read_csv(\"static/data/beaches.csv\")\n",
    "# loop thru beaches and scrape rating information and store as csv\n",
    "for index, row in beaches_df.iterrows():\n",
    "    rank_key.append(row['rank'])\n",
    "    addlink = row['ratingurl']\n",
    "    url = prefix + addlink\n",
    "\n",
    "    # setup Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url)\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    raters = soup(html, 'html.parser')\n",
    "    browser.quit()\n",
    "    time.sleep(15)\n",
    "\n",
    "    # extract data and populate arrays\n",
    "    # get overall rating (has decimals)\n",
    "    rating_text = raters.find(class_=\"biGQs _P fiohW hzzSG uuBRH\").text\n",
    "    rating = float(rating_text)\n",
    "    rates.append(rating)\n",
    "\n",
    "    # get review count\n",
    "    rcount_text = raters.find(class_=\"yyzcQ\").text\n",
    "    # remove all ',' separators and cast as int\n",
    "    rcount_text = rcount_text.replace(',', '')\n",
    "    rvcount.append(int(rcount_text))  \n",
    "\n",
    "    # get the different types of ratings and their values\n",
    "    v1 = raters.find_all(class_=\"IMmqe\")\n",
    "    for i in v1:\n",
    "        rv = i.find(class_=\"biGQs _P pZUbB osNWb\").text\n",
    "        rv = rv.replace(',','') \n",
    "        rt = i.find(class_=\"biGQs _P pZUbB hmDzD\").text\n",
    "\n",
    "        if rt == \"Excellent\":\n",
    "            exclnt.append(rv)\n",
    "        elif rt == 'Very good':\n",
    "            vgood.append(rv)\n",
    "        elif rt == 'Average':\n",
    "            averg.append(rv)\n",
    "        elif rt == 'Poor':\n",
    "            poor.append(rv)\n",
    "        elif rt == 'Terrible':\n",
    "            trrbl.append(rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe and save as csv\n",
    "catlist = np.repeat([\"beaches\"], len(rank_key))\n",
    "df1 = pd.DataFrame({\"category\": catlist, \"rank\": rank_key, \"rate\": rates, \"total_reviews\": rvcount, \"excellent\": exclnt, \n",
    "                    \"very_good\": vgood, \"average\": averg, \"poor\": poor, \"terrible\": trrbl})\n",
    "# save dataframe as csv \n",
    "filename = \"static/data/beachreviews.csv\"\n",
    "df1.to_csv(filename, encoding=\"utf-8\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa6173",
   "metadata": {},
   "source": [
    "### Scraping Reviews for Things to do Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb33c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "rvcount = []\n",
    "exclnt = []\n",
    "vgood = []\n",
    "averg = []\n",
    "poor = []\n",
    "trrbl = []\n",
    "rank_key = []\n",
    "\n",
    "# Get rating and misc info for all things-to-do\n",
    "prefix = \"https://www.tripadvisor.com\"\n",
    "\n",
    "beaches_df = pd.read_csv(\"static/data/things.csv\")\n",
    "# loop thru things-to-do and scrape rating information and store as csv\n",
    "for index, row in beaches_df.iterrows():\n",
    "    rank_key.append(row['rank'])\n",
    "    addlink = row['ratingurl']\n",
    "    url = prefix + addlink\n",
    "\n",
    "    # setup Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url)\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    raters = soup(html, 'html.parser')\n",
    "    browser.quit()\n",
    "    time.sleep(15)\n",
    "\n",
    "    # extract data and populate arrays\n",
    "    # get overall rating (has decimals)\n",
    "    rating_text = raters.find(class_=\"biGQs _P fiohW hzzSG uuBRH\").text\n",
    "    rating = float(rating_text)\n",
    "    rates.append(rating)\n",
    "\n",
    "    # get review count\n",
    "    rary_text = raters.find_all('span', class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    rcount_text = rary_text[-1].text\n",
    "\n",
    "    # remove all ',' separators and cast as int\n",
    "    rcount_text = rcount_text.replace(',', '')\n",
    "    rcount_text = rcount_text.replace(' reviews', '')\n",
    "    rvcount.append(int(rcount_text))  \n",
    "\n",
    "    # get the different types of ratings and their values\n",
    "    v1 = raters.find_all(class_=\"IMmqe\")\n",
    "    for i in v1:\n",
    "        rv = i.find(class_=\"biGQs _P pZUbB osNWb\").text  \n",
    "        rv = rv.replace(',','') \n",
    "        rt = i.find(class_=\"biGQs _P pZUbB hmDzD\").text \n",
    "\n",
    "        if rt == \"Excellent\":\n",
    "            exclnt.append(rv)\n",
    "        elif rt == 'Very good':\n",
    "            vgood.append(rv)\n",
    "        elif rt == 'Average':\n",
    "            averg.append(rv)\n",
    "        elif rt == 'Poor':\n",
    "            poor.append(rv)\n",
    "        elif rt == 'Terrible':\n",
    "            trrbl.append(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42380879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe and save as csv\n",
    "catlist = np.repeat([\"things\"], len(rank_key))\n",
    "df2 = pd.DataFrame({\"category\": catlist, \"rank\": rank_key, \"rate\": rates, \"total_reviews\": rvcount, \"excellent\": exclnt, \n",
    "                    \"very_good\": vgood, \"average\": averg, \"poor\": poor, \"terrible\": trrbl})\n",
    "# save dataframe as csv \n",
    "filename = \"static/data/thingsreviews.csv\"\n",
    "df2.to_csv(filename, encoding=\"utf-8\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b05a4",
   "metadata": {},
   "source": [
    "### Scraping reviews for Restaurants category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d12c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "rvcount = []\n",
    "exclnt = []\n",
    "vgood = []\n",
    "averg = []\n",
    "poor = []\n",
    "trrbl = []\n",
    "rank_key = []\n",
    "\n",
    "# Get rating and misc info for all restaurants\n",
    "prefix = \"https://www.tripadvisor.com\"\n",
    "\n",
    "inns_df = pd.read_csv(\"static/data/restaurants.csv\")\n",
    "# loop thru restaurants and scrape rating information and store as csv\n",
    "for index, row in inns_df.iterrows():\n",
    "    rank_key.append(row['rank'])\n",
    "    addlink = row['ratingurl']\n",
    "    url = prefix + addlink\n",
    "\n",
    "    # setup Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url)\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    raters = soup(html, 'html.parser')\n",
    "    browser.quit()\n",
    "    time.sleep(15)\n",
    "\n",
    "    # extract data and populate arrays\n",
    "    # get overall rating (has decimals)\n",
    "    rating_text = raters.find('span', class_=\"ZDEqb\").text\n",
    "    rating = float(rating_text)\n",
    "    rates.append(rating)\n",
    "\n",
    "    # get review count\n",
    "    rary_text = raters.find('a', class_=\"IcelI\").text\n",
    "    # remove all ',' separators and cast as int\n",
    "    rcount_text = rary_text.replace(',', '')\n",
    "    rcount_text = rcount_text.replace(' reviews', '')\n",
    "    rvcount.append(int(rcount_text))  \n",
    "\n",
    "    # get the different types of ratings and their values\n",
    "    v1 = raters.find_all(class_=\"row_num is-shown-at-tablet\")\n",
    "    count = 0\n",
    "    for i in v1:\n",
    "        rv = i.text\n",
    "        count += 1\n",
    "        if count == 1:\n",
    "            rt = 'Excellent'\n",
    "            exclnt.append(rv)\n",
    "        elif count == 2:\n",
    "            rt = 'Very good'\n",
    "            vgood.append(rv)\n",
    "        elif count == 3:\n",
    "            rt = 'Average'\n",
    "            averg.append(rv)\n",
    "        elif count == 4:\n",
    "            rt = 'Poor'\n",
    "            poor.append(rv)\n",
    "        elif count == 5:\n",
    "            rt = 'Terrible'\n",
    "            trrbl.append(rv)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a04f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe and save as csv\n",
    "catlist = np.repeat([\"restaurants\"], len(rank_key))\n",
    "df3 = pd.DataFrame({\"category\": catlist, \"rank\": rank_key, \"rate\": rates, \"total_reviews\": rvcount, \"excellent\": exclnt, \n",
    "                    \"very_good\": vgood, \"average\": averg, \"poor\": poor, \"terrible\": trrbl})\n",
    "# save dataframe as csv \n",
    "filename = \"static/data/restaurantreviews.csv\"\n",
    "df3.to_csv(filename, encoding=\"utf-8\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbdb4a",
   "metadata": {},
   "source": [
    "### Scrapping Reviews for Hotels category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de8dd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "rvcount = []\n",
    "exclnt = []\n",
    "vgood = []\n",
    "averg = []\n",
    "poor = []\n",
    "trrbl = []\n",
    "rank_key = []\n",
    "\n",
    "# Get rating and misc info for all restaurants\n",
    "prefix = \"https://www.tripadvisor.com\"\n",
    "\n",
    "inns_df = pd.read_csv(\"static/data/hotels.csv\")\n",
    "# loop thru restaurants and scrape rating information and store as csv\n",
    "for index, row in inns_df.iterrows():\n",
    "    rank_key.append(row['rank'])\n",
    "    addlink = row['ratingurl']\n",
    "    url_h = prefix + addlink\n",
    "\n",
    "    # setup Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url_h)\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    raters = soup(html, 'html.parser')\n",
    "    browser.quit()\n",
    "    time.sleep(5)\n",
    "\n",
    "    # extract data and populate arrays\n",
    "    # get overall rating (has decimals)\n",
    "    rating_text = raters.find('span', class_=\"uwJeR P\").text\n",
    "    rating = float(rating_text)\n",
    "    rates.append(rating)\n",
    "\n",
    "    # get review count\n",
    "    rvwcount_text = raters.find('span', class_=\"hkxYU q Wi z Wc\").text    \n",
    "    # remove all ',' separators and cast as int\n",
    "    rcount_text = rvwcount_text.replace(',', '')\n",
    "    rcount_text = rcount_text.replace(' reviews', '')\n",
    "    rvcount.append(int(rcount_text))  \n",
    "\n",
    "    # get the different types of ratings and their values\n",
    "    v1 = raters.find_all(class_=\"NLuQa\")\n",
    "    count = 0\n",
    "    for i in v1:\n",
    "        rv = i.text\n",
    "        count += 1\n",
    "        if count == 1:\n",
    "            rt = 'Excellent'\n",
    "            exclnt.append(rv)\n",
    "        elif count == 2:\n",
    "            rt = 'Very good'\n",
    "            vgood.append(rv)\n",
    "        elif count == 3:\n",
    "            rt = 'Average'\n",
    "            averg.append(rv)\n",
    "        elif count == 4:\n",
    "            rt = 'Poor'\n",
    "            poor.append(rv)\n",
    "        elif count == 5:\n",
    "            rt = 'Terrible'\n",
    "            trrbl.append(rv)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf6fb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe and save as csv\n",
    "catlist = np.repeat([\"hotels\"], len(rank_key))\n",
    "df3 = pd.DataFrame({\"category\": catlist, \"rank\": rank_key, \"rate\": rates, \"total_reviews\": rvcount, \"excellent\": exclnt, \n",
    "                    \"very_good\": vgood, \"average\": averg, \"poor\": poor, \"terrible\": trrbl})\n",
    "# save dataframe as csv \n",
    "filename = \"static/data/hotelreviews.csv\"\n",
    "df3.to_csv(filename, encoding=\"utf-8\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456bbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "65d580feb761fe2ba8e44a47e5fcfa7ab48146f0cbe36200890e98e34de43db0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
